import argparse
import logging
import numpy as np
import pandas as pd
import scipy.stats as st
from pathlib import Path
from scipy.stats import kstest
from typing import List, Dict, Any  # <--- ì´ ë¶€ë¶„ì´ ë¹ ì ¸ ìˆì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
import warnings

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
warnings.filterwarnings('ignore')

logging.basicConfig(
    level=logging.INFO,
    format="[%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

# ë¶„ì„í•  í›„ë³´ ë¶„í¬ ë¦¬ìŠ¤íŠ¸
CANDIDATE_DISTS = [
    "expon",        # ì§€ìˆ˜ ë¶„í¬
    "lognorm",      # ë¡œê·¸ ì •ê·œ ë¶„í¬
    "gamma",        # ê°ë§ˆ ë¶„í¬
    "weibull_min",  # ì™€ì´ë¸” ë¶„í¬
    "pareto",       # íŒŒë ˆí†  ë¶„í¬
    "uniform",      # ê· ë“± ë¶„í¬
]

# -----------------------------------------------------------------------------
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# -----------------------------------------------------------------------------

def ensure_positive(x: np.ndarray, eps: float = 1e-9) -> np.ndarray:
    """ë°ì´í„° ì–‘ìˆ˜ ë³´ì •"""
    x = np.asarray(x, dtype=float)
    x = x[x >= 0]
    return np.maximum(x, eps)

def calculate_score(data: np.ndarray, dist_name: str, params: tuple) -> float:
    """
    ì í•©ë„ ì ìˆ˜ ê³„ì‚° (R-squared ê¸°ë°˜, 0~100ì )
    """
    n = len(data)
    if n < 2: return 0.0
    
    x_sorted = np.sort(data)
    y_empirical = np.arange(1, n + 1) / n
    
    dist = getattr(st, dist_name)
    try:
        y_theoretical = dist.cdf(x_sorted, *params)
    except:
        return 0.0
    
    ss_res = np.sum((y_empirical - y_theoretical) ** 2)
    ss_tot = np.sum((y_empirical - np.mean(y_empirical)) ** 2)
    
    if ss_tot == 0: return 0.0
    
    r2 = 1 - (ss_res / ss_tot)
    return max(r2, 0.0) * 100.0

# -----------------------------------------------------------------------------
# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# -----------------------------------------------------------------------------

def compute_level_gaps(level_df: pd.DataFrame) -> np.ndarray:
    sub = level_df.dropna(subset=["min_key", "max_key"])
    if len(sub) < 2: return np.array([])
    
    sub["min_key"] = pd.to_numeric(sub["min_key"], errors="coerce")
    sub["max_key"] = pd.to_numeric(sub["max_key"], errors="coerce")
    sub = sub.dropna().sort_values(["min_key", "max_key"])
    
    gaps = sub["min_key"].values[1:] - sub["max_key"].values[:-1]
    return np.maximum(gaps, 0.0)

def compute_key_density(level_df: pd.DataFrame) -> np.ndarray:
    sub = level_df.dropna(subset=["min_key", "max_key", "entry_n"])
    cols = ["min_key", "max_key", "entry_n"]
    for c in cols: sub[c] = pd.to_numeric(sub[c], errors="coerce")
    sub = sub.dropna()
    sub = sub[sub["entry_n"] > 0]
    if len(sub) == 0: return np.array([])
    
    kd = (sub["max_key"].values - sub["min_key"].values + 1.0) / sub["entry_n"].values
    return kd[kd >= 0]

def load_grouped_data(input_dir: str, pattern: str, target_mode: str) -> List[dict]:
    """CSV íŒŒì¼ë“¤ì„ ì½ì–´ (DB, Level)ë³„ ë°ì´í„° ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    in_dir = Path(input_dir)
    files = sorted(in_dir.glob(pattern))
    
    groups = []
    
    logger.info(f"Target: {target_mode.upper()} | {len(files)}ê°œ íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    for csv_path in files:
        try:
            df = pd.read_csv(csv_path)
            if not {"level", "min_key", "max_key"}.issubset(df.columns): continue
            
            db_name = csv_path.stem
            
            for lvl, g in df.groupby("level"):
                if target_mode == "gap":
                    vals = compute_level_gaps(g)
                elif target_mode == "kd":
                    vals = compute_key_density(g)
                else:
                    vals = np.array([])
                
                # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´(ì˜ˆ: 10ê°œ ë¯¸ë§Œ) Fitting ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë¯€ë¡œ ì œì™¸í•  ìˆ˜ë„ ìˆìŒ
                if len(vals) >= 10:
                    groups.append({
                        "id": f"{db_name}_L{int(lvl)}",
                        "data": vals
                    })
        except: continue
        
    return groups

# -----------------------------------------------------------------------------
# 3. í•µì‹¬ ë¡œì§: ëª¨ë“  ê·¸ë£¹ì— ëŒ€í•´ ëª¨ë“  ë¶„í¬ í‰ê°€
# -----------------------------------------------------------------------------

def evaluate_all_groups(groups: List[dict]) -> pd.DataFrame:
    """
    ê° ê·¸ë£¹ë³„ë¡œ ëª¨ë“  í›„ë³´ ë¶„í¬ë¥¼ Fittingí•˜ê³  ì ìˆ˜ë¥¼ ê¸°ë¡
    """
    if not groups:
        return pd.DataFrame()

    logger.info(f"ì´ {len(groups)}ê°œì˜ ê·¸ë£¹ì— ëŒ€í•´ ê°œë³„ Fitting ìˆ˜í–‰ ì¤‘...")
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    # êµ¬ì¡°: [{'group': 'db_L1', 'expon_score': 90, 'gamma_score': 95, ...}, ...]
    all_scores = []

    for idx, grp in enumerate(groups):
        data = ensure_positive(grp['data'])
        row = {"group_id": grp['id'], "sample_count": len(data)}
        
        # ì§„í–‰ ìƒí™© ë¡œê¹… (ë„ˆë¬´ ìì£¼ ì°ì§€ ì•ŠìŒ)
        if idx % 10 == 0:
            logger.debug(f"Processing group {idx+1}/{len(groups)}...")

        best_score_in_group = -1
        best_dist_in_group = None

        for dist_name in CANDIDATE_DISTS:
            try:
                dist = getattr(st, dist_name)
                # Fit
                params = dist.fit(data)
                # Score
                score = calculate_score(data, dist_name, params)
                
                row[f"{dist_name}_score"] = score
                
                # ì´ ê·¸ë£¹ì—ì„œì˜ ìŠ¹ì íŒë³„ìš©
                if score > best_score_in_group:
                    best_score_in_group = score
                    best_dist_in_group = dist_name
                    
            except:
                row[f"{dist_name}_score"] = 0.0
        
        # ì´ ê·¸ë£¹ì˜ Best ë¶„í¬ ê¸°ë¡
        row["winner_dist"] = best_dist_in_group
        all_scores.append(row)

    return pd.DataFrame(all_scores)


def print_global_leaderboard(df_scores: pd.DataFrame):
    """
    ì „ì²´ ê·¸ë£¹ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ 'ìµœì¢… ìš°ìŠ¹ ë¶„í¬' ë­í‚¹ ì¶œë ¥
    """
    if df_scores.empty:
        logger.error("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_groups = len(df_scores)
    
    # ì§‘ê³„ìš© ë¦¬ìŠ¤íŠ¸
    leaderboard = []
    
    for dist_name in CANDIDATE_DISTS:
        col_name = f"{dist_name}_score"
        if col_name not in df_scores.columns: continue
        
        scores = df_scores[col_name]
        
        # 1. í‰ê·  ì ìˆ˜ (Average Score)
        avg_score = scores.mean()
        
        # 2. ìš°ìŠ¹ íšŸìˆ˜ (Win Count) - í•´ë‹¹ ë¶„í¬ê°€ ê·¸ë£¹ ë‚´ 1ë“±ì„ í•œ íšŸìˆ˜
        win_count = len(df_scores[df_scores["winner_dist"] == dist_name])
        win_rate = (win_count / total_groups) * 100
        
        # 3. ì•ˆì •ì„± (90ì  ì´ìƒì¸ ë¹„ìœ¨)
        excellent_rate = (len(scores[scores >= 90]) / total_groups) * 100
        
        leaderboard.append({
            "Distribution": dist_name,
            "Avg Score": avg_score,
            "Win Rate (%)": win_rate,
            "Excellent Fit (>90pt)": excellent_rate
        })
    
    # í‰ê·  ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    df_leaderboard = pd.DataFrame(leaderboard).sort_values(by="Avg Score", ascending=False)
    
    # --- í„°ë¯¸ë„ ì¶œë ¥ ---
    print("\n" + "="*80)
    print(f" ğŸ† FINAL RESULT: Global Best Distribution (Based on {total_groups} Groups)")
    print("="*80)
    print(f" * í•´ì„: 'Avg Score'ê°€ ê°€ì¥ ë†’ì€ ë¶„í¬ê°€ ëª¨ë“  ê·¸ë£¹ì„ í†µí‹€ì–´ ê°€ì¥ ë²”ìš©ì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.")
    print("-" * 80)
    
    # í¬ë§·íŒ…í•˜ì—¬ ì¶œë ¥
    print(f"{'Rank':<5} {'Distribution':<15} {'Avg Score':<12} {'Win Rate(1st)':<15} {'Excellent Fit(%)':<15}")
    print("-" * 80)
    
    for rank, (idx, row) in enumerate(df_leaderboard.iterrows(), 1):
        name = row['Distribution']
        avg = row['Avg Score']
        win = row['Win Rate (%)']
        exc = row['Excellent Fit (>90pt)']
        
        # 1ë“±ì€ ê°•ì¡° í‘œì‹œ
        prefix = "â­ï¸" if rank == 1 else "  "
        print(f"{prefix:<4} {name:<15} {avg:6.2f} pt     {win:5.1f} %        {exc:5.1f} %")
        
    print("="*80 + "\n")


# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Identify the single best distribution across all groups.")
    parser.add_argument("input_dir", help="Directory containing CSV files")
    parser.add_argument("--pattern", default="*.csv", help="File pattern")
    parser.add_argument("--target", choices=["gap", "kd"], default="gap", help="Target metric")
    
    args = parser.parse_args()

    try:
        # 1. ë°ì´í„° ë¡œë“œ (ê·¸ë£¹ë³„ ë¦¬ìŠ¤íŠ¸)
        groups = load_grouped_data(args.input_dir, args.pattern, args.target)
        
        if not groups:
            logger.error("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: ìœ íš¨í•œ ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
            exit(1)
            
        # 2. ëª¨ë“  ê·¸ë£¹ ê°œë³„ í”¼íŒ… & ì ìˆ˜ ê³„ì‚°
        df_results = evaluate_all_groups(groups)
        
        # 3. ì§‘ê³„ ë° ìµœì¢… ë­í‚¹ ì¶œë ¥
        print_global_leaderboard(df_results)
        
        # (ì˜µì…˜) ìƒì„¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # df_results.to_csv(f"fitting_details_{args.target}.csv", index=False)

    except Exception as e:
        logger.error(f"Error occurred: {str(e)}")